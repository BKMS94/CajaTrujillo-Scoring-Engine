# ğŸ›ï¸ Motor de Scoring Crediticio en Tiempo Real - Caja Trujillo

Este repositorio contiene la arquitectura y el cÃ³digo fuente para un motor de decisiones crediticias de baja latencia. El sistema integra el procesamiento distribuido de Big Data con capas de cachÃ© en memoria para ofrecer aprobaciones de crÃ©dito instantÃ¡neas basadas en reglas de negocio dinÃ¡micas y perfiles de riesgo de la SBS.

## ğŸ’¡ Contexto de Negocio

El proyecto resuelve la necesidad de automatizar la evaluaciÃ³n crediticia en la Caja Trujillo. Al migrar de un modelo de evaluaciÃ³n manual a uno basado en **Arquitectura Medallion**, se logra:

* **ReducciÃ³n de Latencia:** Procesamiento de solicitudes en milisegundos mediante una capa de velocidad.
* **Consistencia:** AplicaciÃ³n uniforme de polÃ­ticas de riesgo (reglas SBS).
* **Escalabilidad:** Capacidad para manejar picos de demanda mediante contenedores aislados.

## ğŸš€ Arquitectura del Sistema

La soluciÃ³n emplea un enfoque hÃ­brido de procesamiento por lotes y tiempo real:

1. **Ingesta:** RecepciÃ³n de solicitudes en formato JSON (compatible con aplicaciones mÃ³viles).
2. **Speed Layer (Redis):** CachÃ© de alto rendimiento que almacena los sueldos y perfiles histÃ³ricos para evitar consultas costosas a la base de datos principal durante el scoring.
3. **Motor de Procesamiento (Apache Spark):** Motor encargado de la validaciÃ³n de esquemas, enriquecimiento de datos y aplicaciÃ³n de lÃ³gica condicional (ej. lÃ­mites de endeudamiento del 20% o 30%).
4. **Persistencia (Medallion Architecture):** * **Bronze:** Archivos crudos de ingesta.
    * **Silver:** Datos normalizados y validados.
    * **Gold:** Resultados finales almacenados en **Parquet** particionado por fecha y estado de decisiÃ³n, optimizados para analÃ­tica.

## ğŸ› ï¸ Stack TecnolÃ³gico

* **Core:** Python 3.x, PySpark 3.5
* **NoSQL / Cache:** Redis (Alpine version)
* **Infraestructura:** Docker & Docker Compose
* **Formato de Archivos:** Apache Parquet (Columnar Storage)

## ğŸ“‚ Estructura del Proyecto

```text
.
â”œâ”€â”€ docker-compose.yml       # OrquestaciÃ³n de contenedores (Spark & Redis)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ jobs/
â”‚   â”‚   â””â”€â”€ scoring_job.py   # LÃ³gica principal de procesamiento en Spark
â”‚   â””â”€â”€ database/
â”‚       â””â”€â”€ redis_init.py    # Script de hidrataciÃ³n de datos en cachÃ©
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ bronze/              # Landing zone de datos crudos
â”‚   â”œâ”€â”€ silver/              # Datos transformados
â”‚   â””â”€â”€ gold/                # Tablas finales particionadas
â””â”€â”€ README.md
```
## âš™ï¸ GuÃ­a de EjecuciÃ³n
Siga estos pasos para desplegar el entorno local de pruebas:

1. **Despliegue de Infraestructura**
    Levante los servicios de Spark y Redis en una red virtual aislada:

    ```bash
    docker-compose up -d
    ```

2. **HidrataciÃ³n de la Capa de Velocidad**
    Cargue los datos maestros de clientes en Redis para habilitar el scoring instantÃ¡neo:

    ```bash
    docker exec -it redis_cache python /opt/bitnami/spark/src/database/redis_init.py
    ```

3. **EjecuciÃ³n del Motor de Scoring**
    Inicie el job de Spark para evaluar las solicitudes de crÃ©dito:

    ```bash
    docker exec -it spark_engine spark-submit /opt/bitnami/spark/src/jobs/scoring_job.py
    ```

## ğŸ›¡ï¸ Manejo de Excepciones y Resiliencia
El sistema implementa una Dead Letter Queue (DLQ) lÃ³gica. Cualquier registro que presente inconsistencias en los montos o no cumpla con el contrato de datos (Schema) se deriva automÃ¡ticamente a una ruta de auditorÃ­a manual. Esto asegura que el pipeline principal nunca se detenga y que el Data Lake mantenga solo informaciÃ³n de alta calidad para la toma de decisiones.